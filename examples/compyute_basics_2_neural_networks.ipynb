{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in Compyute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compyute as cp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "`Compyute` offers some helper functions to make data preparation easy. Here we use `split_train_val_test` to split the data into train, val, and test sets. We use `normalize` to normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from compyute.preprocessing import split_train_val_test, normalize\n",
    "\n",
    "# download the data\n",
    "url = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# encode the targets\n",
    "data[\"variety\"] = data[\"variety\"].astype(\"category\").cat.codes\n",
    "data_tensor = cp.tensor(data.to_numpy())\n",
    "\n",
    "# split the data into train, val, test\n",
    "train, val, test = split_train_val_test(data_tensor, ratio_val=0.25, ratio_test=0.25)\n",
    "\n",
    "# split features from targets\n",
    "X_train, y_train = train[:, :-1], train[:, -1].to_int()\n",
    "X_val, y_val = val[:, :-1], val[:, -1].to_int()\n",
    "X_test, y_test = test[:, :-1], test[:, -1].to_int()\n",
    "\n",
    "# normalize features\n",
    "X_train = normalize(X_train, dim=0, low=-1, high=1)\n",
    "X_val = normalize(X_val, dim=0, low=-1, high=1)\n",
    "X_test = normalize(X_test, dim=0, low=-1, high=1)\n",
    "\n",
    "print (f'{X_train.shape=}')\n",
    "print (f'{y_train.shape=}')\n",
    "print (f'{X_val.shape=}')\n",
    "print (f'{y_val.shape=}')\n",
    "print (f'{X_test.shape=}')\n",
    "print (f'{y_test.shape=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network Structure\n",
    "Here the individual layers of the neural network models are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_channels=4, out_channels=16),  # 4 input nodes, 16 neurons\n",
    "    nn.ReLU(),  # activation function\n",
    "    nn.Linear(in_channels=16, out_channels=3),  # 16 input nodes, 3 neurons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = cp.nn.utils.get_module_summary(model, input_shape=(4,))\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "To train a model, the easiest way is to use the `Trainer` object. Here we use `sgd` as an optimizer, `cross_entropy` as a loss function, and `accuracy` as a metric. Furthermore, we use the `History` callback to track the loss and accuracy values during training. We also use the `ProgressBar` callback to track the progress of the training process.\n",
    "\n",
    "Note: If you want to customize e.g. the optimizer with a different learning rate, instead of defining the optimizer via a string, you can also pass an optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.nn.trainer import Trainer\n",
    "from compyute.nn.trainer.callbacks import History, ProgressBar\n",
    "\n",
    "history = History()\n",
    "prograss = ProgressBar(mode=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=\"sgd\",  # alternatively, pass in an object, e.g. nn.optimizers.SGD()\n",
    "    loss=\"cross_entropy\",\n",
    "    metric=\"accuracy\",\n",
    "    callbacks=[history, prograss]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(X_train, y_train, epochs=10000, batch_size=256, val_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(t1, t2):\n",
    "    trace1 = history[t1]\n",
    "    trace2 = history[t2]\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(cp.arange(start=1, stop=len(trace1) + 1), trace1, linewidth=1, label=t1)\n",
    "    plt.plot(cp.arange(start=1, stop=len(trace2) + 1), trace2, linewidth=1, label=t2)\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss/accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "plot_history(\"loss\", \"accuracy_score\");\n",
    "plot_history(\"val_loss\", \"val_accuracy_score\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Using the defined metric, the model's performance can be evaluated using testing/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = trainer.evaluate_model(X_test, y_test)\n",
    "print(f'loss {loss:.4f}')\n",
    "print(f'accuracy {100 * accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained Model\n",
    "\n",
    "Models are saved by serializing the state dictionary of the model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"model\": model.get_state_dict(),\n",
    "    \"optimizer\": trainer.optimizer.get_state_dict()\n",
    "}\n",
    "cp.save(state, \"iris_model.cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved model, you have to instanciate a new model and load the state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute import nn\n",
    "\n",
    "# create a model and optimizer instance\n",
    "loaded_model = nn.Sequential(\n",
    "    nn.Linear(in_channels=4, out_channels=16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_channels=16, out_channels=3),\n",
    ")\n",
    "loaded_optimizer = nn.optimizers.SGD(loaded_model.get_parameters())\n",
    "\n",
    "# load parameters from the checkpoint\n",
    "loaded_state = cp.load(\"iris_model.cp\")\n",
    "loaded_model.load_state_dict(loaded_state[\"model\"])\n",
    "loaded_optimizer.load_state_dict(loaded_state[\"optimizer\"])\n",
    "\n",
    "# test the loaded model\n",
    "sample = cp.tensor([[5.1, 3.5, 1.4, 0.2]])\n",
    "loaded_model(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deee277ef8cb4a05cf6441d551c854fa5e547ddedbca2c10e6f5685ea62b6c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
